{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428b584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef61aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers rank_bm25 PyMuPDF python-telegram-bot==13.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f08876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import fitz\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e54240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_with_fonts(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text_data = []\n",
    "\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc.load_page(page_num)\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "\n",
    "        for block in blocks:\n",
    "            if \"lines\" in block:\n",
    "                for line in block[\"lines\"]:\n",
    "                    for span in line[\"spans\"]:\n",
    "                        text_data.append({\n",
    "                            \"text\": span[\"text\"],\n",
    "                            \"size\": span[\"size\"],\n",
    "                            \"flags\" : span[\"flags\"],\n",
    "                            \"color\" : span[\"color\"]\n",
    "                        })\n",
    "    return text_data\n",
    "\n",
    "def parse_text_with_fonts(text_data):\n",
    "    data = []\n",
    "    chapter = None\n",
    "    title = None\n",
    "    subtitle = None\n",
    "    headline = None\n",
    "    comment = None\n",
    "    contents = \"\"\n",
    "    current_size = None\n",
    "    current_flag = None\n",
    "    chapter_comp = False\n",
    "\n",
    "    for item in text_data:\n",
    "        text = item[\"text\"].strip()\n",
    "        size = int(item[\"size\"])\n",
    "        flags = int(item[\"flags\"])\n",
    "        color = int(item[\"color\"])\n",
    "        chapter_match = re.match(r'Chapter (\\d+)\\. (.+)', text)\n",
    "\n",
    "        if flags == 16 and size >= 14:\n",
    "            if (chapter_match or chapter_comp) and size > 18 and color == 0:\n",
    "                if chapter and contents != \"\":\n",
    "                    data.append([chapter, title, subtitle, headline, comment, contents.strip()])\n",
    "\n",
    "                if size == current_size and current_flag == flags and chapter:\n",
    "                    chapter = chapter + \" \" + text\n",
    "                else:\n",
    "                    chapter = text\n",
    "\n",
    "                title = None\n",
    "                subtitle = None\n",
    "                headline = None\n",
    "                comment = None\n",
    "                contents = \"\"\n",
    "                current_size = size\n",
    "            elif size >= 18 and (not color == 0):\n",
    "                if chapter and contents != \"\":\n",
    "                    data.append([chapter, title, subtitle, headline, comment, contents.strip()])\n",
    "\n",
    "                if size == current_size and current_flag == flags and title:\n",
    "                    title = title + \" \" + text\n",
    "                else:\n",
    "                    title = text\n",
    "\n",
    "                subtitle = None\n",
    "                headline = None\n",
    "                comment = None\n",
    "                contents = \"\"\n",
    "                current_size = size\n",
    "                \n",
    "            elif text.isupper():\n",
    "                if chapter and contents != \"\":\n",
    "                    data.append([chapter, title, subtitle, headline, comment, contents.strip()])\n",
    "                    \n",
    "                comment = text\n",
    "                contents = \"\"\n",
    "                current_size = size\n",
    "                \n",
    "            elif 14 <= size <18 and color ==0:\n",
    "                if chapter and contents != \"\":\n",
    "                    data.append([chapter, title, subtitle, headline, comment, contents.strip()])\n",
    "\n",
    "                if size == current_size and current_flag == flags and subtitle:\n",
    "                    subtitle = subtitle + \" \" + text\n",
    "                else:\n",
    "                    subtitle = text\n",
    "\n",
    "                headline = None\n",
    "                comment = None\n",
    "                contents = \"\"\n",
    "                current_size = size\n",
    "\n",
    "            elif 15 <= size <16 and (not color == 0):\n",
    "                if chapter and contents != \"\":\n",
    "                    data.append([chapter, title, subtitle, headline, comment, contents.strip()])\n",
    "\n",
    "                if size == current_size and current_flag == flags and headline:\n",
    "                    headline = headline + \" \" + text\n",
    "                else:\n",
    "                    headline = text\n",
    "                comment = None\n",
    "                contents = \"\"\n",
    "                current_size = size\n",
    "            current_flag = 16\n",
    "\n",
    "        elif size <= 16:\n",
    "            contents = contents + \" \" + text\n",
    "            current_size = size\n",
    "\n",
    "        if chapter_match:\n",
    "            chapter_comp = True\n",
    "        else:\n",
    "            chapter_comp = False\n",
    "        current_flag = flags\n",
    "\n",
    "    if chapter and contents != \"\":\n",
    "        data.append([chapter, title, subtitle, headline, comment, contents.strip()])\n",
    "    return data\n",
    "\n",
    "text_data = extract_text_with_fonts(\"HML.pdf\")\n",
    "parsed_data = parse_text_with_fonts(text_data)\n",
    "\n",
    "df = pd.DataFrame(parsed_data, columns=['Chapter', 'Title', \"Subtitle\", \"Headline\", \"Comment\", 'Contents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4593d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7eab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rows = []\n",
    "\n",
    "for i in df[\"Title\"].unique():\n",
    "    filtered_subtitles = df[(df[\"Title\"] == i) & (~df[\"Subtitle\"].isin([None]))][\"Subtitle\"].unique()\n",
    "    if filtered_subtitles.size > 0:\n",
    "        ch = df[df[\"Title\"] == i]['Chapter'].iloc[0]  \n",
    "        str_con = \", \".join(filtered_subtitles)\n",
    "        new_rows.append({'Chapter': ch, 'Title': i, 'Subtitle': None, \"Headline\": None, \"Comment\" : None, \"Contents\": str_con})\n",
    "\n",
    "for i in df[\"Subtitle\"].unique():\n",
    "    filtered_headlines = df[(df[\"Subtitle\"] == i) & (~df[\"Headline\"].isin([None]))][\"Headline\"].unique()\n",
    "    if filtered_headlines.size > 0:\n",
    "        ch = df[df[\"Subtitle\"] == i]['Chapter'].iloc[0]\n",
    "        t = df[df[\"Subtitle\"] == i]['Title'].iloc[0]\n",
    "        str_con = \", \".join(filtered_headlines)\n",
    "        new_rows.append({'Chapter': ch, 'Title': t, 'Subtitle': i, \"Headline\": None, \"Comment\" : None, \"Contents\": str_con})\n",
    "\n",
    "new = pd.DataFrame(new_rows)\n",
    "df = pd.concat([df, new], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d08de",
   "metadata": {},
   "outputs": [],
   "source": [
    "new[new['Subtitle'] == \"Training Supervision\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809e252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Title'] == \"Boosting\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d009c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "js_data = df.to_json()\n",
    "json_data = json.loads(js_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161d6e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "for key, value in json_data['Title'].items():\n",
    "    if value is not None:\n",
    "        if not (value in titles):\n",
    "            titles.append(value)\n",
    "        else:\n",
    "            titles.append(\"\")\n",
    "    else:\n",
    "        titles.append(\"\")\n",
    "subtitles = []\n",
    "for key, value in json_data['Subtitle'].items():\n",
    "    if value is not None:\n",
    "        if not (value in subtitles):\n",
    "            subtitles.append(value)\n",
    "        else:\n",
    "            subtitles.append(\"\")\n",
    "    else:\n",
    "        subtitles.append(\"\")\n",
    "headlines = [value if value is not None else \"\" for key, value in json_data['Headline'].items()]\n",
    "contents = [value for key, value in json_data['Contents'].items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb929bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "tokenized_contents = [tokenizer.tokenize(content) for content in contents]\n",
    "bm25_contents = BM25Okapi(tokenized_contents)\n",
    "combined_texts = [f\"{title} {subtitle} {headline}\" for title, subtitle, headline in zip(titles, subtitles, headlines)]\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(combined_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3245adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query):\n",
    "    tokenized_query = tokenizer.tokenize(query)\n",
    "    contents_scores = bm25_contents.get_scores(tokenized_query)\n",
    "    \n",
    "    best_contents_idx, _ = sorted(enumerate(contents_scores), key=lambda x: x[1], reverse=True)[0]\n",
    "    matched_content_c = contents[best_contents_idx]\n",
    "    \n",
    "    query_vector = vectorizer.transform([query])\n",
    "    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    best_index = np.argmax(cosine_similarities)\n",
    "    matched_content = contents[best_index]\n",
    "\n",
    "    def get_answer_from_context(context):\n",
    "        encoded_text = tokenizer.encode_plus(\n",
    "            text=query, text_pair=context, max_length=512, truncation=True,\n",
    "            padding='max_length', return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        inputs = encoded_text['input_ids']\n",
    "        token_type_ids = encoded_text['token_type_ids']\n",
    "        attention_mask = encoded_text['attention_mask']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=inputs, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "\n",
    "        start_logits = outputs.start_logits\n",
    "        end_logits = outputs.end_logits\n",
    "\n",
    "        start_index = torch.argmax(start_logits)\n",
    "        end_index = torch.argmax(end_logits)\n",
    "\n",
    "        if start_index >= end_index:\n",
    "            return \"Sorry, I couldn't find an answer\"\n",
    "        \n",
    "        answer_tokens = inputs.squeeze().tolist()[start_index:end_index + 1]\n",
    "        answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        corrected_ans = ' '.join([word[2:] if word.startswith('##') else word for word in answer.split()])\n",
    "        return corrected_ans.strip()\n",
    "    \n",
    "    answer_from_tf_idf = get_answer_from_context(matched_content)\n",
    "    if answer_from_tf_idf != \"Sorry, I couldn't find an answer\":\n",
    "        return answer_from_tf_idf\n",
    "    else:\n",
    "        answer_from_c = get_answer_from_context(matched_content_c)\n",
    "    \n",
    "    return answer_from_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e489e348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "queries = [\"What is boosting?\", \"What is gradient boosting?\", \"What are the types of Machine learning?\", \"What are the things typically used to detect tumors in brain scans?\", \"What happen in supervised learning?\", \"What are the types of learning?\"]\n",
    "data = {\"Question\": queries, \"Answer\": [process_query(q) for q in queries]}\n",
    "test = pd.DataFrame(data)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb203719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from telegram import Update\n",
    "from telegram.ext import Updater, CommandHandler, MessageHandler, Filters, CallbackContext\n",
    "\n",
    "TOKEN = '7360551119:AAED2_VS4T6OWLg1fdNpjgZb4ydSNfEr8uQ'\n",
    "\n",
    "def start(update: Update, context: CallbackContext):\n",
    "    update.message.reply_text('Welcome!\\nAsk me any question you want to know from the Hands-on Machine Learning book')\n",
    "\n",
    "def handle_message(update: Update, context: CallbackContext):\n",
    "    update.message.reply_text(f'{process_query(update.message.text)}')\n",
    "updater = Updater(token=TOKEN, use_context=True)\n",
    "dispatcher = updater.dispatcher\n",
    "start_handler = CommandHandler('start', start)\n",
    "dispatcher.add_handler(start_handler)\n",
    "message_handler = MessageHandler(Filters.text & ~Filters.command, handle_message)\n",
    "dispatcher.add_handler(message_handler)\n",
    "updater.start_polling()\n",
    "updater.idle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ef0d60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
